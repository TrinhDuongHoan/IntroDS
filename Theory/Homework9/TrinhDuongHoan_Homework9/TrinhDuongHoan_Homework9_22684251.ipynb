{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bài 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "School\t\t\tGrade\tStudent number\t\tName\t\t\tScore\n",
      "Riverdale High         \t1         \t0              \tPhoebe              \t3\n",
      "Riverdale High         \t1         \t1              \tRachel              \t7\n",
      "Riverdale High         \t2         \t0              \tAngela              \t6\n",
      "Riverdale High         \t2         \t1              \tTristan             \t3\n",
      "Riverdale High         \t2         \t2              \tAurora              \t9\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Hàm để lấy dữ liệu từ file input\n",
    "def get_data_from_file():\n",
    "    with open(\"SampleInput.txt\", 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Hàm để chuyển đổi dữ liệu sang định dạng yêu cầu\n",
    "def convert_data(data):\n",
    "    # Tìm tất cả các cặp dữ liệu Grade và các dữ liệu liên quan\n",
    "    grade_data = re.findall(r'Grade = (\\d+)(.*?)\\n\\n\\n|$', data, re.DOTALL)\n",
    "    # print(grade_data)\n",
    "    \n",
    "    result = []\n",
    "    grade_data = grade_data[:-2]\n",
    "    \n",
    "    for grade_match in grade_data:\n",
    "        grade = grade_match[0]\n",
    "        students = re.findall(r'\\n(\\d+), (\\w+)', grade_match[1], re.MULTILINE)\n",
    "\n",
    "        len_ = len(students)\n",
    "        \n",
    "        for i in range(len_ // 2):\n",
    "            result.append((grade, students[i][0], students[i][1], students[i + len_ // 2][1]))\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Hàm để in ra dữ liệu theo định dạng bảng\n",
    "def print_data(data, school_data):\n",
    "    print(\"School\\t\\t\\tGrade\\tStudent number\\t\\tName\\t\\t\\tScore\")\n",
    "    current_grade = ''\n",
    "    for grade, number, name, score in data:\n",
    "        if grade:\n",
    "            current_grade = grade\n",
    "        print(\"{:<23}\\t{:<10}\\t{:<15}\\t{:<20}\\t{}\".format(school_data, current_grade, number, name, score))\n",
    "\n",
    "# Đường dẫn tới file input\n",
    "file_path = \"SampleInput.txt\"\n",
    "\n",
    "# Lấy dữ liệu từ file\n",
    "input_data = get_data_from_file()\n",
    "\n",
    "# Chuyển đổi dữ liệu\n",
    "converted_data = convert_data(input_data)\n",
    "\n",
    "school_data = re.findall(r'School = (.*?)\\n|$', input_data, re.DOTALL)\n",
    "\n",
    "# In ra dữ liệu theo định dạng bảng\n",
    "print_data(converted_data, school_data[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bài 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"labeledTrainData.xlsx.tsv\", sep='\\t')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1  Loại bỏ các thẻ HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    return soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<br /><br />This movie is full of references. Like \\Mad Max II\\\", \\\"The wild one\\\" and many others. The ladybug´s face it´s a clear reference (or tribute) to Peter Lorre. This movie is a masterpiece. We´ll talk much more about in the future.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trinh\\AppData\\Local\\Temp\\ipykernel_9292\\2125600460.py:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, 'html.parser')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie is full of references. Like \\Mad Max II\\\", \\\"The wild one\\\" and many others. The ladybug´s face it´s a clear reference (or tribute) to Peter Lorre. This movie is a masterpiece. We´ll talk much more about in the future.\"\n"
     ]
    }
   ],
   "source": [
    "print(df['review'].iloc[9])\n",
    "df['review'] = df['review'].apply(remove_html_tags)\n",
    "print(df['review'].iloc[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2  Loại bỏ các non-letters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_non_letter(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before remove non-letter: \n",
      "This movie is full of references. Like \\Mad Max II\\\", \\\"The wild one\\\" and many others. The ladybug´s face it´s a clear reference (or tribute) to Peter Lorre. This movie is a masterpiece. We´ll talk much more about in the future.\"\n",
      "After remove non-letter: \n",
      "This movie is full of references Like Mad Max II The wild one and many others The ladybugs face its a clear reference or tribute to Peter Lorre This movie is a masterpiece Well talk much more about in the future\n"
     ]
    }
   ],
   "source": [
    "print(\"Before remove non-letter: \")\n",
    "print(df['review'].iloc[9])\n",
    "df['review'] = df['review'].apply(remove_non_letter)\n",
    "print(\"After remove non-letter: \")\n",
    "print(df['review'].iloc[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3  Chuyển các từ thành chữ thường và tách thành các từ riêng biệt với giả thuyết là các \n",
    "từ cách nhau bởi khoảng trắng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_and_split(text):\n",
    "    lowercase_text = text.lower()\n",
    "    words = lowercase_text.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: \n",
      "This movie is full of references Like Mad Max II The wild one and many others The ladybugs face its a clear reference or tribute to Peter Lorre This movie is a masterpiece Well talk much more about in the future\n",
      "After: \n",
      "['this', 'movie', 'is', 'full', 'of', 'references', 'like', 'mad', 'max', 'ii', 'the', 'wild', 'one', 'and', 'many', 'others', 'the', 'ladybugs', 'face', 'its', 'a', 'clear', 'reference', 'or', 'tribute', 'to', 'peter', 'lorre', 'this', 'movie', 'is', 'a', 'masterpiece', 'well', 'talk', 'much', 'more', 'about', 'in', 'the', 'future']\n"
     ]
    }
   ],
   "source": [
    "print(\"Before: \")\n",
    "print(df['review'].iloc[9])\n",
    "df['review'] = df['review'].apply(lower_and_split)\n",
    "print(\"After: \")\n",
    "print(df['review'].iloc[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4   Loại bỏ các stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\trinh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\trinh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    words = text\n",
    "    \n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    # filtered_text = ' '.join(filtered_words)\n",
    "    \n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: \n",
      "['this', 'movie', 'is', 'full', 'of', 'references', 'like', 'mad', 'max', 'ii', 'the', 'wild', 'one', 'and', 'many', 'others', 'the', 'ladybugs', 'face', 'its', 'a', 'clear', 'reference', 'or', 'tribute', 'to', 'peter', 'lorre', 'this', 'movie', 'is', 'a', 'masterpiece', 'well', 'talk', 'much', 'more', 'about', 'in', 'the', 'future']\n",
      "After: \n",
      "['movie', 'full', 'references', 'like', 'mad', 'max', 'ii', 'wild', 'one', 'many', 'others', 'ladybugs', 'face', 'clear', 'reference', 'tribute', 'peter', 'lorre', 'movie', 'masterpiece', 'well', 'talk', 'much', 'future']\n"
     ]
    }
   ],
   "source": [
    "print(\"Before: \")\n",
    "print(df['review'].iloc[9])\n",
    "df['review'] = df['review'].apply(remove_stopwords)\n",
    "print(\"After: \")\n",
    "print(df['review'].iloc[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5   Chuyển các từ về từ gốc: Stem words (sử dụng thư viện SnowballStemmer của nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(text):\n",
    "    stemmer = SnowballStemmer(language='english')\n",
    "    text = ' '.join(text)\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # stemmed_text = ' '.join(stemmed_words)\n",
    "    \n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: \n",
      "['movie', 'full', 'references', 'like', 'mad', 'max', 'ii', 'wild', 'one', 'many', 'others', 'ladybugs', 'face', 'clear', 'reference', 'tribute', 'peter', 'lorre', 'movie', 'masterpiece', 'well', 'talk', 'much', 'future']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After: \n",
      "['movi', 'full', 'refer', 'like', 'mad', 'max', 'ii', 'wild', 'one', 'mani', 'other', 'ladybug', 'face', 'clear', 'refer', 'tribut', 'peter', 'lorr', 'movi', 'masterpiec', 'well', 'talk', 'much', 'futur']\n"
     ]
    }
   ],
   "source": [
    "print(\"Before: \")\n",
    "print(df['review'].iloc[9])\n",
    "df['review'] = df['review'].apply(stem_words)\n",
    "print(\"After: \")\n",
    "print(df['review'].iloc[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.6   Nối các từ thành một chuỗi, các từ cách nhau bởi khoảng trắng và trả về kết quả "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_words(words):\n",
    "    result = ' '.join(words)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: \n",
      "['movi', 'full', 'refer', 'like', 'mad', 'max', 'ii', 'wild', 'one', 'mani', 'other', 'ladybug', 'face', 'clear', 'refer', 'tribut', 'peter', 'lorr', 'movi', 'masterpiec', 'well', 'talk', 'much', 'futur']\n",
      "After: \n",
      "movi full refer like mad max ii wild one mani other ladybug face clear refer tribut peter lorr movi masterpiec well talk much futur\n"
     ]
    }
   ],
   "source": [
    "print(\"Before: \")\n",
    "print(df['review'].iloc[9])\n",
    "df['review'] = df['review'].apply(join_words)\n",
    "print(\"After: \")\n",
    "print(df['review'].iloc[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.7   Chuyển thành đặc trưng TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of TF-IDF matrix: (25000, 112296)\n",
      "Vocabulary: ['00' '000' '0000000000001' ... 'zzzzz' 'zzzzzzzz'\n",
      " 'zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz']\n",
      "TF-IDF matrix:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['review'])\n",
    "\n",
    "print(\"Shape of TF-IDF matrix:\", tfidf_matrix.shape)\n",
    "\n",
    "print(\"Vocabulary:\", tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"TF-IDF matrix:\")\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.8   Tính độ đo tương đồng Cosin dựa trên đặc trưng TF-IDF của 2 câu bất kỳ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform([df['review'].iloc[8], df['review'].iloc[9]])\n",
    "cosine_sim = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between the two sentences: 0.008417265597487362\n"
     ]
    }
   ],
   "source": [
    "print(\"Cosine similarity between the two sentences:\", cosine_sim[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
